{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:01:08.836719Z","iopub.execute_input":"2025-04-15T20:01:08.836982Z","iopub.status.idle":"2025-04-15T20:01:12.971925Z","shell.execute_reply.started":"2025-04-15T20:01:08.836962Z","shell.execute_reply":"2025-04-15T20:01:12.971165Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport numpy as np\nimport zipfile\nimport requests\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\n# import torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import random_split, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nfrom pathlib import Path\nimport json\nimport wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:23:22.574882Z","iopub.execute_input":"2025-04-16T13:23:22.575445Z","iopub.status.idle":"2025-04-16T13:23:33.223786Z","shell.execute_reply.started":"2025-04-16T13:23:22.575419Z","shell.execute_reply":"2025-04-16T13:23:33.223250Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_url = \"https://storage.googleapis.com/wandb_datasets/nature_12K.zip\"\ndataset_zip_path = \"/kaggle/working/nature_12K.zip\"\ndataset_dir = \"nature_12K\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:23:33.225249Z","iopub.execute_input":"2025-04-16T13:23:33.225467Z","iopub.status.idle":"2025-04-16T13:23:33.229010Z","shell.execute_reply.started":"2025-04-16T13:23:33.225450Z","shell.execute_reply":"2025-04-16T13:23:33.228349Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"if not os.path.exists(dataset_dir):\n    if not os.path.exists(dataset_zip_path):\n        print(\"Downloading iNaturalist-12K...\")\n        response = requests.get(dataset_url, stream=True)\n        total_size = int(response.headers.get('content-length', 0))\n        with open(dataset_zip_path, 'wb') as f, tqdm(\n            desc=dataset_zip_path,\n            total=total_size,\n            unit='iB',\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as bar:\n            for data in response.iter_content(chunk_size=1024):\n                size = f.write(data)\n                bar.update(size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:23:33.232068Z","iopub.execute_input":"2025-04-16T13:23:33.232281Z","iopub.status.idle":"2025-04-16T13:24:12.115640Z","shell.execute_reply.started":"2025-04-16T13:23:33.232267Z","shell.execute_reply":"2025-04-16T13:24:12.114945Z"}},"outputs":[{"name":"stdout","text":"Downloading iNaturalist-12K...\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/nature_12K.zip: 100%|██████████| 3.55G/3.55G [00:38<00:00, 98.4MiB/s]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"if not os.path.exists(dataset_dir):\n    print(\"Extracting dataset...\")\n    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n        zip_ref.extractall(\".\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:25:47.838641Z","iopub.execute_input":"2025-04-16T13:25:47.838928Z","iopub.status.idle":"2025-04-16T13:26:03.636459Z","shell.execute_reply.started":"2025-04-16T13:25:47.838888Z","shell.execute_reply":"2025-04-16T13:26:03.635824Z"}},"outputs":[{"name":"stdout","text":"Extracting dataset...\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def prepare_datasets(data_dir, val_split=0.2, batch_size=32, image_size=(224, 224)):\n    data_dir = Path(data_dir)\n\n    # Define transforms (customize as needed)\n    transform = transforms.Compose([\n        transforms.Resize(image_size),\n        transforms.ToTensor(),  # Converts to [0, 1] and CxHxW\n        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3),  # Normalize RGB\n    ])\n\n    # Load training and testing datasets\n    full_train_dataset = ImageFolder(root=data_dir / \"train\", transform=transform)\n    test_dataset = ImageFolder(root=data_dir / \"val\", transform=transform)\n\n    # Create validation split from training set\n    val_size = int(val_split * len(full_train_dataset))\n    train_size = len(full_train_dataset) - val_size\n\n    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n\n    # Extract X and Y by iterating over DataLoader batches if needed\n    train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n\n    # Convert to X, Y tensors\n    def extract_XY(loader):\n        for X, Y in loader:\n            return X, Y\n\n    X_train, Y_train = extract_XY(train_loader)\n    X_val, Y_val = extract_XY(val_loader)\n    X_test, Y_test = extract_XY(test_loader)\n\n    return X_train, Y_train, X_val, Y_val, X_test, Y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:26:03.637404Z","iopub.execute_input":"2025-04-16T13:26:03.637639Z","iopub.status.idle":"2025-04-16T13:26:03.644148Z","shell.execute_reply.started":"2025-04-16T13:26:03.637618Z","shell.execute_reply":"2025-04-16T13:26:03.643370Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# dataset_dir = \"/kaggle/working/inaturalist_12K\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:19:39.645427Z","iopub.execute_input":"2025-04-16T13:19:39.646118Z","iopub.status.idle":"2025-04-16T13:19:39.649525Z","shell.execute_reply.started":"2025-04-16T13:19:39.646071Z","shell.execute_reply":"2025-04-16T13:19:39.648735Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# X_train, Y_train, X_val, Y_val, X_test, Y_test = prepare_datasets(\n#     data_dir='/kaggle/working/inaturalist_12K', val_split=0.2, batch_size=64, image_size=(224, 224)\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(X_train.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T20:04:55.066926Z","iopub.status.idle":"2025-04-15T20:04:55.067365Z","shell.execute_reply.started":"2025-04-15T20:04:55.067159Z","shell.execute_reply":"2025-04-15T20:04:55.067172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part A\n### Question 1\n\nBuild a small CNN model consisting of 5 convolution layers. Each convolution layer would be followed by an activation and a max-pooling layer.\n\nAfter 5 such conv-activation-maxpool blocks, you should have one dense layer followed by the output layer containing 10 neurons. The input layer should be compatible with the images in the iNaturalist dataset dataset.\nThe code should be flexible such that the number of filters, size of filters, and activation function of the convolution layers and dense layers can be changed. You should also be able to change the number of neurons in the dense layer.","metadata":{}},{"cell_type":"code","source":"api_key = \"7040d84a3ed65a967eb3389dd6fe774b418576ed\" \nwandb.login(key=api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:25:32.352146Z","iopub.execute_input":"2025-04-16T13:25:32.352436Z","iopub.status.idle":"2025-04-16T13:25:32.596351Z","shell.execute_reply.started":"2025-04-16T13:25:32.352417Z","shell.execute_reply":"2025-04-16T13:25:32.595710Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"class FlexibleCNN(nn.Module):\n    def __init__(self, \n                 num_filters=32,  # number of filters in each conv layer\n                 filter_size=3,   # size of filters (k x k)\n                 activation='relu',  # activation function\n                 dense_neurons=512,  # number of neurons in dense layer\n                 input_channels=3,   # RGB images\n                 num_classes=10,    # number of output classes\n                 use_batch_norm=True,  # whether to use batch normalization\n                 dropout_rate=0.2):  # dropout rate\n        super(FlexibleCNN, self).__init__()\n        \n        # Store parameters for calculations\n        self.num_filters = num_filters\n        self.filter_size = filter_size\n        self.dense_neurons = dense_neurons\n        self.use_batch_norm = use_batch_norm\n        self.dropout_rate = dropout_rate\n        \n        # Choose activation function\n        if activation.lower() == 'relu':\n            self.activation = nn.ReLU()\n        elif activation.lower() == 'leakyrelu':\n            self.activation = nn.LeakyReLU()\n        elif activation.lower() == 'gelu':\n            self.activation = nn.GELU()\n        elif activation.lower() == 'silu':\n            self.activation = nn.SiLU()\n        elif activation.lower() == 'mish':\n            self.activation = nn.Mish()\n        else:\n            raise ValueError(f\"Unsupported activation function: {activation}\")\n        \n        # Create 5 conv-activation-maxpool blocks\n        self.conv_blocks = nn.ModuleList()\n        in_channels = input_channels\n        \n        for _ in range(5):\n            block = []\n            # Conv layer\n            block.append(nn.Conv2d(in_channels, num_filters, filter_size, padding=filter_size//2))\n            \n            # Batch normalization if enabled\n            if use_batch_norm:\n                block.append(nn.BatchNorm2d(num_filters))\n            \n            # Activation\n            block.append(self.activation)\n            \n            # Max pooling\n            block.append(nn.MaxPool2d(2, 2))\n            \n            # Dropout after pooling\n            block.append(nn.Dropout2d(dropout_rate))\n            \n            self.conv_blocks.extend(block)\n            in_channels = num_filters\n        \n        # Calculate the size of the flattened features after conv blocks\n        # Assuming input size of 224x224 (standard for iNaturalist)\n        self.flattened_size = num_filters * (224 // (2**5)) * (224 // (2**5))\n        \n        # Dense layers\n        self.dense = nn.Sequential(\n            nn.Linear(self.flattened_size, dense_neurons),\n            self.activation,\n            nn.Dropout(dropout_rate),  # Dropout before final layer\n            nn.Linear(dense_neurons, num_classes)\n        )\n    \n    def forward(self, x):\n        for block in self.conv_blocks:\n            x = block(x)\n        x = x.view(x.size(0), -1)\n        x = self.dense(x)\n        return x\n    \n    def get_computations(self):\n        \"\"\"Calculate total number of computations\"\"\"\n        # Computations in conv layers\n        conv_computations = 0\n        input_size = 224\n        in_channels = 3\n        \n        for i in range(5):\n            # Each conv layer\n            conv_computations += (input_size * input_size * in_channels * \n                                self.num_filters * self.filter_size * self.filter_size)\n            # Each maxpool reduces size by 2\n            input_size = input_size // 2\n            in_channels = self.num_filters\n        \n        # Computations in dense layers\n        dense_computations = (self.flattened_size * self.dense_neurons +  # first dense layer\n                            self.dense_neurons * 10)  # output layer\n        \n        return conv_computations + dense_computations\n    \n    def get_parameters(self):\n        \"\"\"Calculate total number of parameters\"\"\"\n        # Parameters in conv layers\n        conv_params = 0\n        in_channels = 3\n        \n        for _ in range(5):\n            # Each conv layer has (filter_size * filter_size * in_channels + 1) * num_filters parameters\n            conv_params += (self.filter_size * self.filter_size * in_channels + 1) * self.num_filters\n            # Batch norm parameters if enabled\n            if self.use_batch_norm:\n                conv_params += 2 * self.num_filters  # gamma and beta for each channel\n            in_channels = self.num_filters\n        \n        # Parameters in dense layers\n        dense_params = (self.flattened_size * self.dense_neurons + self.dense_neurons +  # first dense layer\n                       self.dense_neurons * 10 + 10)  # output layer\n        \n        return conv_params + dense_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:25:37.083985Z","iopub.execute_input":"2025-04-16T13:25:37.084313Z","iopub.status.idle":"2025-04-16T13:25:37.095600Z","shell.execute_reply.started":"2025-04-16T13:25:37.084295Z","shell.execute_reply":"2025-04-16T13:25:37.094681Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train(config=None):\n    # Initialize wandb\n    with wandb.init(config=config):\n        config = wandb.config\n        \n        # Set random seed for reproducibility\n        torch.manual_seed(config.seed)\n        np.random.seed(config.seed)\n        \n        # Data augmentation and normalization\n        train_transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                              std=[0.229, 0.224, 0.225])\n        ])\n        \n        val_transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                              std=[0.229, 0.224, 0.225])\n        ])\n        \n        # Load dataset\n        train_dataset = datasets.ImageFolder(\n            root='/kaggle/working/inaturalist_12K/train',\n            transform=train_transform\n        )\n        \n        # Split into train and validation\n        train_size = int(0.8 * len(train_dataset))\n        val_size = len(train_dataset) - train_size\n        \n        # Use stratified split to maintain class balance\n        train_indices, val_indices = train_test_split(\n            list(range(len(train_dataset))),\n            test_size=0.2,\n            stratify=train_dataset.targets,\n            random_state=config.seed\n        )\n        \n        train_subset = torch.utils.data.Subset(train_dataset, train_indices)\n        val_subset = torch.utils.data.Subset(train_dataset, val_indices)\n        \n        train_loader = DataLoader(\n            train_subset,\n            batch_size=config.batch_size,\n            shuffle=True,\n            num_workers=4\n        )\n        \n        val_loader = DataLoader(\n            val_subset,\n            batch_size=config.batch_size,\n            shuffle=False,\n            num_workers=4\n        )\n        \n        # Initialize model\n        model = FlexibleCNN(\n            num_filters=config.num_filters,\n            filter_size=config.filter_size,\n            activation=config.activation,\n            dense_neurons=config.dense_neurons,\n            use_batch_norm=config.use_batch_norm,\n            dropout_rate=config.dropout_rate\n        )\n        \n        # Move model to GPU if available\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        model = model.to(device)\n        \n        # Loss function and optimizer\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n\n        # Format the run name\n        run_name = f\"nf_{config.num_filters}_fs_{config.filter_size}_act_{config.activation}_dn_{config.dense_neurons}_lr_{config.learning_rate}_bs_{config.batch_size}_bn_{config.use_batch_norm}_dr_{config.dropout_rate}\"\n\n        # Set the run name\n        wandb.run.name = run_name\n        \n        # Training loop\n        best_val_acc = 0\n        best_config = None\n        \n        for epoch in range(config.epochs):\n            # Training phase\n            model.train()\n            train_loss = 0\n            train_correct = 0\n            train_total = 0\n            \n            for batch_idx, (inputs, targets) in enumerate(train_loader):\n                inputs, targets = inputs.to(device), targets.to(device)\n                \n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                loss.backward()\n                optimizer.step()\n                \n                train_loss += loss.item()\n                _, predicted = outputs.max(1)\n                train_total += targets.size(0)\n                train_correct += predicted.eq(targets).sum().item()\n                \n                if batch_idx % 100 == 0:\n                    print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n            \n            train_acc = 100. * train_correct / train_total\n            \n            # Validation phase\n            model.eval()\n            val_loss = 0\n            val_correct = 0\n            val_total = 0\n            \n            with torch.no_grad():\n                for inputs, targets in val_loader:\n                    inputs, targets = inputs.to(device), targets.to(device)\n                    outputs = model(inputs)\n                    loss = criterion(outputs, targets)\n                    \n                    val_loss += loss.item()\n                    _, predicted = outputs.max(1)\n                    val_total += targets.size(0)\n                    val_correct += predicted.eq(targets).sum().item()\n            \n            val_acc = 100. * val_correct / val_total\n            \n            # Log metrics to wandb\n            wandb.log({\n                \"epoch\": epoch,\n                \"train_loss\": train_loss / len(train_loader),\n                \"train_acc\": train_acc,\n                \"val_loss\": val_loss / len(val_loader),\n                \"val_acc\": val_acc\n            })\n            \n            # Save best model\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                torch.save(model.state_dict(), 'best_model.pth')\n                best_config = dict(config)\n                with open('best_config.json', 'w') as f:\n                    json.dump(best_config, f, indent=4)\n            \n            print(f'Epoch: {epoch}, Train Loss: {train_loss/len(train_loader):.4f}, '\n                  f'Train Acc: {train_acc:.2f}%, Val Loss: {val_loss/len(val_loader):.4f}, '\n                  f'Val Acc: {val_acc:.2f}%')\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:27:03.698102Z","iopub.execute_input":"2025-04-16T13:27:03.698753Z","iopub.status.idle":"2025-04-16T13:27:03.712007Z","shell.execute_reply.started":"2025-04-16T13:27:03.698730Z","shell.execute_reply":"2025-04-16T13:27:03.711185Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\n# Define sweep configuration\nsweep_config = {\n    'method': 'bayes',  # Use Bayesian optimization\n    'metric': {\n        'name': 'val_acc',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'num_filters': {\n            'values': [16, 32, 64]\n        },\n        'filter_size': {\n            'values': [3]\n        },\n        'activation': {\n            'values': ['relu', 'gelu', 'silu', 'mish']\n        },\n        'dense_neurons': {\n            'values': [128, 256]\n        },\n        'learning_rate': {\n            'min': 1e-4,\n            'max': 1e-2\n        },\n        'batch_size': {\n            'values': [32, 64, 128]\n        },\n        'use_batch_norm': {\n            'values': [True, False]\n        },\n        'dropout_rate': {\n            'values': [0.2, 0.3, 0.4]\n        },\n        'epochs': {\n            'value': 5\n        },\n        'seed': {\n            'value': 42\n        }\n    }\n}\n\n# Initialize sweep\nsweep_id = wandb.sweep(sweep_config, project=\"inaturalist-cnn-sweep_01\")\n\n# Run sweep\nwandb.agent(sweep_id, train, count=50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T13:27:04.224872Z","iopub.execute_input":"2025-04-16T13:27:04.225530Z","iopub.status.idle":"2025-04-16T15:46:22.025503Z","shell.execute_reply.started":"2025-04-16T13:27:04.225509Z","shell.execute_reply":"2025-04-16T15:46:22.024934Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: wnhzz4d3\nSweep URL: https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ghgl3oih with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.009635104324700636\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_132710-ghgl3oih</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ghgl3oih' target=\"_blank\">breezy-sweep-1</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ghgl3oih' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ghgl3oih</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 0, Loss: 2.2914\nEpoch: 0, Batch: 100, Loss: 2.2797\nEpoch: 0, Batch: 200, Loss: 2.4909\nEpoch: 0, Train Loss: 2.6229, Train Acc: 9.78%, Val Loss: 2.3030, Val Acc: 10.00%\nEpoch: 1, Batch: 0, Loss: 2.3090\nEpoch: 1, Batch: 100, Loss: 2.3093\nEpoch: 1, Batch: 200, Loss: 2.3099\nEpoch: 1, Train Loss: 2.4006, Train Acc: 9.61%, Val Loss: 2.3029, Val Acc: 10.00%\nEpoch: 2, Batch: 0, Loss: 2.3024\nEpoch: 2, Batch: 100, Loss: 2.3005\nEpoch: 2, Batch: 200, Loss: 2.3098\nEpoch: 2, Train Loss: 2.3064, Train Acc: 9.61%, Val Loss: 2.3030, Val Acc: 10.00%\nEpoch: 3, Batch: 0, Loss: 2.3018\nEpoch: 3, Batch: 100, Loss: 2.2997\nEpoch: 3, Batch: 200, Loss: 2.2988\nEpoch: 3, Train Loss: 3.1968, Train Acc: 9.53%, Val Loss: 2.3032, Val Acc: 10.00%\nEpoch: 4, Batch: 0, Loss: 2.3102\nEpoch: 4, Batch: 100, Loss: 2.3021\nEpoch: 4, Batch: 200, Loss: 2.3079\nEpoch: 4, Train Loss: 2.3181, Train Acc: 9.90%, Val Loss: 2.3028, Val Acc: 10.00%\nEpoch: 5, Batch: 0, Loss: 2.2966\nEpoch: 5, Batch: 100, Loss: 2.3036\nEpoch: 5, Batch: 200, Loss: 2.3001\nEpoch: 5, Train Loss: 2.3051, Train Acc: 9.89%, Val Loss: 2.3029, Val Acc: 10.00%\nEpoch: 6, Batch: 0, Loss: 2.2970\nEpoch: 6, Batch: 100, Loss: 2.3139\nEpoch: 6, Batch: 200, Loss: 2.2956\nEpoch: 6, Train Loss: 2.3043, Train Acc: 9.99%, Val Loss: 2.3033, Val Acc: 10.00%\nEpoch: 7, Batch: 0, Loss: 2.3118\nEpoch: 7, Batch: 100, Loss: 2.3208\nEpoch: 7, Batch: 200, Loss: 2.2932\nEpoch: 7, Train Loss: 2.3045, Train Acc: 9.71%, Val Loss: 2.3036, Val Acc: 10.00%\nEpoch: 8, Batch: 0, Loss: 2.3038\nEpoch: 8, Batch: 100, Loss: 2.3048\nEpoch: 8, Batch: 200, Loss: 2.3078\nEpoch: 8, Train Loss: 2.3042, Train Acc: 9.36%, Val Loss: 2.3040, Val Acc: 10.00%\nEpoch: 9, Batch: 0, Loss: 2.3133\nEpoch: 9, Batch: 100, Loss: 2.3508\nEpoch: 9, Batch: 200, Loss: 2.3082\nEpoch: 9, Train Loss: 2.3340, Train Acc: 9.76%, Val Loss: 2.3032, Val Acc: 10.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▆▄▄▃▇▇█▅▁▅</td></tr><tr><td>train_loss</td><td>▃▂▁█▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂▂▂▃▁▂▄▆█▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>9.76372</td></tr><tr><td>train_loss</td><td>2.33398</td></tr><tr><td>val_acc</td><td>10</td></tr><tr><td>val_loss</td><td>2.30317</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">breezy-sweep-1</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ghgl3oih' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ghgl3oih</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_132710-ghgl3oih/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cgmmjsp7 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00618197211365554\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_133634-cgmmjsp7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/cgmmjsp7' target=\"_blank\">soft-sweep-2</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/cgmmjsp7' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/cgmmjsp7</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 0, Loss: 2.3080\nEpoch: 0, Batch: 100, Loss: 2.4672\nEpoch: 0, Train Loss: 2.4512, Train Acc: 9.91%, Val Loss: 2.3026, Val Acc: 10.00%\nEpoch: 1, Batch: 0, Loss: 2.3937\nEpoch: 1, Batch: 100, Loss: 2.3022\nEpoch: 1, Train Loss: 2.3412, Train Acc: 9.75%, Val Loss: 2.3027, Val Acc: 10.00%\nEpoch: 2, Batch: 0, Loss: 2.3018\nEpoch: 2, Batch: 100, Loss: 2.3021\nEpoch: 2, Train Loss: 2.3052, Train Acc: 9.86%, Val Loss: 2.3027, Val Acc: 10.00%\nEpoch: 3, Batch: 0, Loss: 2.3104\nEpoch: 3, Batch: 100, Loss: 2.3017\nEpoch: 3, Train Loss: 2.3629, Train Acc: 9.60%, Val Loss: 2.3027, Val Acc: 10.00%\nEpoch: 4, Batch: 0, Loss: 2.3100\nEpoch: 4, Batch: 100, Loss: 2.3014\nEpoch: 4, Train Loss: 2.4566, Train Acc: 10.00%, Val Loss: 2.3027, Val Acc: 10.00%\nEpoch: 5, Batch: 0, Loss: 2.3131\nEpoch: 5, Batch: 100, Loss: 21.4079\nEpoch: 5, Train Loss: 14.5881, Train Acc: 9.25%, Val Loss: 2.3026, Val Acc: 10.00%\nEpoch: 6, Batch: 0, Loss: 2.3049\nEpoch: 6, Batch: 100, Loss: 2.2989\nEpoch: 6, Train Loss: 20.8900, Train Acc: 9.60%, Val Loss: 2.3027, Val Acc: 10.00%\nEpoch: 7, Batch: 0, Loss: 2.3128\nEpoch: 7, Batch: 100, Loss: 2.3099\nEpoch: 7, Train Loss: 3.2822, Train Acc: 9.93%, Val Loss: 2.3028, Val Acc: 10.00%\nEpoch: 8, Batch: 0, Loss: 2.3002\nEpoch: 8, Batch: 100, Loss: 2.3074\nEpoch: 8, Train Loss: 25.6921, Train Acc: 10.01%, Val Loss: 2.3027, Val Acc: 10.00%\nEpoch: 9, Batch: 0, Loss: 2.3060\nEpoch: 9, Batch: 100, Loss: 2.3039\nEpoch: 9, Train Loss: 62.6072, Train Acc: 9.33%, Val Loss: 2.3029, Val Acc: 10.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▇▆▇▄█▁▄▇█▂</td></tr><tr><td>train_loss</td><td>▁▁▁▁▁▂▃▁▄█</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▄▄▂▄▁▃▅▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>9.32617</td></tr><tr><td>train_loss</td><td>62.60718</td></tr><tr><td>val_acc</td><td>10</td></tr><tr><td>val_loss</td><td>2.30294</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">soft-sweep-2</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/cgmmjsp7' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/cgmmjsp7</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_133634-cgmmjsp7/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3w9ms6zx with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.007719484424177824\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_134544-3w9ms6zx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/3w9ms6zx' target=\"_blank\">vague-sweep-3</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/3w9ms6zx' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/3w9ms6zx</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 0, Loss: 2.3340\nEpoch: 0, Batch: 100, Loss: 2.3018\nEpoch: 0, Train Loss: 2.4807, Train Acc: 11.80%, Val Loss: 2.2901, Val Acc: 14.50%\nEpoch: 1, Batch: 0, Loss: 2.2377\nEpoch: 1, Batch: 100, Loss: 2.2998\nEpoch: 1, Train Loss: 2.3105, Train Acc: 11.20%, Val Loss: 2.2822, Val Acc: 10.60%\nEpoch: 2, Batch: 0, Loss: 2.2831\nEpoch: 2, Batch: 100, Loss: 2.3098\nEpoch: 2, Train Loss: 2.3039, Train Acc: 11.78%, Val Loss: 2.2813, Val Acc: 12.50%\nEpoch: 3, Batch: 0, Loss: 2.2756\nEpoch: 3, Batch: 100, Loss: 2.3477\nEpoch: 3, Train Loss: 2.3038, Train Acc: 11.54%, Val Loss: 2.2995, Val Acc: 14.00%\nEpoch: 4, Batch: 0, Loss: 2.2533\nEpoch: 4, Batch: 100, Loss: 2.2869\nEpoch: 4, Train Loss: 2.2928, Train Acc: 12.63%, Val Loss: 2.2499, Val Acc: 13.95%\nEpoch: 5, Batch: 0, Loss: 2.2998\nEpoch: 5, Batch: 100, Loss: 2.2764\nEpoch: 5, Train Loss: 2.2805, Train Acc: 13.86%, Val Loss: 2.2443, Val Acc: 15.25%\nEpoch: 6, Batch: 0, Loss: 2.3262\nEpoch: 6, Batch: 100, Loss: 2.3193\nEpoch: 6, Train Loss: 2.2750, Train Acc: 13.20%, Val Loss: 2.2491, Val Acc: 14.45%\nEpoch: 7, Batch: 0, Loss: 2.2607\nEpoch: 7, Batch: 100, Loss: 2.2773\nEpoch: 7, Train Loss: 2.2729, Train Acc: 13.91%, Val Loss: 2.2233, Val Acc: 16.55%\nEpoch: 8, Batch: 0, Loss: 2.3003\nEpoch: 8, Batch: 100, Loss: 2.3225\nEpoch: 8, Train Loss: 2.2606, Train Acc: 14.29%, Val Loss: 2.2524, Val Acc: 15.35%\nEpoch: 9, Batch: 0, Loss: 2.2989\nEpoch: 9, Batch: 100, Loss: 2.1999\nEpoch: 9, Train Loss: 2.2634, Train Acc: 13.90%, Val Loss: 2.2489, Val Acc: 16.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▂▁▂▂▄▇▆▇█▇</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_acc</td><td>▆▁▃▅▅▆▆█▇▇</td></tr><tr><td>val_loss</td><td>▇▆▆█▃▃▃▁▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>13.90174</td></tr><tr><td>train_loss</td><td>2.26344</td></tr><tr><td>val_acc</td><td>16</td></tr><tr><td>val_loss</td><td>2.24888</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vague-sweep-3</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/3w9ms6zx' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/3w9ms6zx</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_134544-3w9ms6zx/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l0up09y8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.003963434717984732\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_135503-l0up09y8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/l0up09y8' target=\"_blank\">silver-sweep-4</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/l0up09y8' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/l0up09y8</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 0, Loss: 2.3323\nEpoch: 0, Batch: 100, Loss: 2.3674\nEpoch: 0, Batch: 200, Loss: 2.2847\nEpoch: 0, Train Loss: 2.3801, Train Acc: 12.30%, Val Loss: 2.2421, Val Acc: 16.20%\nEpoch: 1, Batch: 0, Loss: 2.1428\nEpoch: 1, Batch: 100, Loss: 2.2653\nEpoch: 1, Batch: 200, Loss: 2.3876\nEpoch: 1, Train Loss: 2.2875, Train Acc: 13.09%, Val Loss: 2.2587, Val Acc: 12.20%\nEpoch: 2, Batch: 0, Loss: 2.2048\nEpoch: 2, Batch: 100, Loss: 2.1963\nEpoch: 2, Batch: 200, Loss: 2.2424\nEpoch: 2, Train Loss: 2.2907, Train Acc: 12.76%, Val Loss: 2.2575, Val Acc: 14.05%\nEpoch: 3, Batch: 0, Loss: 2.3074\nEpoch: 3, Batch: 100, Loss: 2.2902\nEpoch: 3, Batch: 200, Loss: 2.3886\nEpoch: 3, Train Loss: 2.2828, Train Acc: 14.04%, Val Loss: 2.2288, Val Acc: 17.85%\nEpoch: 4, Batch: 0, Loss: 2.2403\nEpoch: 4, Batch: 100, Loss: 2.2109\nEpoch: 4, Batch: 200, Loss: 2.1271\nEpoch: 4, Train Loss: 2.2639, Train Acc: 15.21%, Val Loss: 2.1944, Val Acc: 19.80%\nEpoch: 5, Batch: 0, Loss: 2.2794\nEpoch: 5, Batch: 100, Loss: 2.2802\nEpoch: 5, Batch: 200, Loss: 2.4875\nEpoch: 5, Train Loss: 2.2528, Train Acc: 16.19%, Val Loss: 2.2053, Val Acc: 17.90%\nEpoch: 6, Batch: 0, Loss: 2.1659\nEpoch: 6, Batch: 100, Loss: 2.3727\nEpoch: 6, Batch: 200, Loss: 2.3147\nEpoch: 6, Train Loss: 2.2408, Train Acc: 16.38%, Val Loss: 2.1871, Val Acc: 19.55%\nEpoch: 7, Batch: 0, Loss: 2.2404\nEpoch: 7, Batch: 100, Loss: 2.2309\nEpoch: 7, Batch: 200, Loss: 2.2105\nEpoch: 7, Train Loss: 2.2230, Train Acc: 17.23%, Val Loss: 2.1460, Val Acc: 21.25%\nEpoch: 8, Batch: 0, Loss: 2.3696\nEpoch: 8, Batch: 100, Loss: 2.3162\nEpoch: 8, Batch: 200, Loss: 2.1925\nEpoch: 8, Train Loss: 2.2261, Train Acc: 17.28%, Val Loss: 2.1493, Val Acc: 21.05%\nEpoch: 9, Batch: 0, Loss: 2.1058\nEpoch: 9, Batch: 100, Loss: 2.3599\nEpoch: 9, Batch: 200, Loss: 2.2914\nEpoch: 9, Train Loss: 2.2051, Train Acc: 18.00%, Val Loss: 2.1265, Val Acc: 21.20%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▂▂▃▅▆▆▇▇█</td></tr><tr><td>train_loss</td><td>█▄▄▄▃▃▂▂▂▁</td></tr><tr><td>val_acc</td><td>▄▁▂▅▇▅▇███</td></tr><tr><td>val_loss</td><td>▇██▆▅▅▄▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>18.00225</td></tr><tr><td>train_loss</td><td>2.20505</td></tr><tr><td>val_acc</td><td>21.2</td></tr><tr><td>val_loss</td><td>2.12649</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">silver-sweep-4</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/l0up09y8' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/l0up09y8</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_135503-l0up09y8/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7xp0ff3y with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.008043417747210263\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_140437-7xp0ff3y</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/7xp0ff3y' target=\"_blank\">eternal-sweep-5</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/7xp0ff3y' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/7xp0ff3y</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 0, Loss: 2.3035\nEpoch: 0, Batch: 100, Loss: 2.2028\nEpoch: 0, Batch: 200, Loss: 2.2678\nEpoch: 0, Train Loss: 2.5072, Train Acc: 10.71%, Val Loss: 2.3021, Val Acc: 9.80%\nEpoch: 1, Batch: 0, Loss: 2.3579\nEpoch: 1, Batch: 100, Loss: 2.2959\nEpoch: 1, Batch: 200, Loss: 2.2506\nEpoch: 1, Train Loss: 2.3193, Train Acc: 11.09%, Val Loss: 2.3031, Val Acc: 11.50%\nEpoch: 2, Batch: 0, Loss: 2.3470\nEpoch: 2, Batch: 100, Loss: 2.3156\nEpoch: 2, Batch: 200, Loss: 2.3414\nEpoch: 2, Train Loss: 2.3162, Train Acc: 10.34%, Val Loss: 2.3018, Val Acc: 9.90%\nEpoch: 3, Batch: 0, Loss: 2.2542\nEpoch: 3, Batch: 100, Loss: 2.3043\nEpoch: 3, Batch: 200, Loss: 2.3545\nEpoch: 3, Train Loss: 2.3109, Train Acc: 10.24%, Val Loss: 2.3030, Val Acc: 10.00%\nEpoch: 4, Batch: 0, Loss: 2.2962\nEpoch: 4, Batch: 100, Loss: 2.2959\nEpoch: 4, Batch: 200, Loss: 2.3099\nEpoch: 4, Train Loss: 2.3047, Train Acc: 10.03%, Val Loss: 2.3024, Val Acc: 11.35%\nEpoch: 5, Batch: 0, Loss: 2.3019\nEpoch: 5, Batch: 100, Loss: 2.3039\nEpoch: 5, Batch: 200, Loss: 2.2963\nEpoch: 5, Train Loss: 2.3116, Train Acc: 9.71%, Val Loss: 2.3041, Val Acc: 10.75%\nEpoch: 6, Batch: 0, Loss: 2.3172\nEpoch: 6, Batch: 100, Loss: 2.3038\nEpoch: 6, Batch: 200, Loss: 2.3116\nEpoch: 6, Train Loss: 2.3079, Train Acc: 9.64%, Val Loss: 2.3025, Val Acc: 11.10%\nEpoch: 7, Batch: 0, Loss: 2.3011\nEpoch: 7, Batch: 100, Loss: 2.3074\nEpoch: 7, Batch: 200, Loss: 2.2998\nEpoch: 7, Train Loss: 2.3067, Train Acc: 10.08%, Val Loss: 2.3024, Val Acc: 9.95%\nEpoch: 8, Batch: 0, Loss: 2.3063\nEpoch: 8, Batch: 100, Loss: 2.3099\nEpoch: 8, Batch: 200, Loss: 2.3095\nEpoch: 8, Train Loss: 2.3069, Train Acc: 9.80%, Val Loss: 2.3002, Val Acc: 10.00%\nEpoch: 9, Batch: 0, Loss: 2.2923\nEpoch: 9, Batch: 100, Loss: 2.3018\nEpoch: 9, Batch: 200, Loss: 2.3098\nEpoch: 9, Train Loss: 2.3158, Train Acc: 9.56%, Val Loss: 2.3009, Val Acc: 11.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▆█▅▄▃▂▁▃▂▁</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁█▁▂▇▅▆▂▂▆</td></tr><tr><td>val_loss</td><td>▄▆▄▆▅█▅▅▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>9.5637</td></tr><tr><td>train_loss</td><td>2.31575</td></tr><tr><td>val_acc</td><td>11</td></tr><tr><td>val_loss</td><td>2.30094</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">eternal-sweep-5</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/7xp0ff3y' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/7xp0ff3y</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_140437-7xp0ff3y/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fu33zw1k with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.002225779837107186\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_141341-fu33zw1k</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/fu33zw1k' target=\"_blank\">clear-sweep-6</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/fu33zw1k' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/fu33zw1k</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 0, Loss: 2.3679\nEpoch: 0, Batch: 100, Loss: 2.1949\nEpoch: 0, Train Loss: 2.2951, Train Acc: 16.19%, Val Loss: 2.1561, Val Acc: 21.90%\nEpoch: 1, Batch: 0, Loss: 2.2305\nEpoch: 1, Batch: 100, Loss: 2.2967\nEpoch: 1, Train Loss: 2.1842, Train Acc: 20.09%, Val Loss: 2.1032, Val Acc: 22.80%\nEpoch: 2, Batch: 0, Loss: 2.2473\nEpoch: 2, Batch: 100, Loss: 2.1583\nEpoch: 2, Train Loss: 2.1640, Train Acc: 20.94%, Val Loss: 2.0704, Val Acc: 26.10%\nEpoch: 3, Batch: 0, Loss: 2.2010\nEpoch: 3, Batch: 100, Loss: 2.4965\nEpoch: 3, Train Loss: 2.1327, Train Acc: 22.27%, Val Loss: 2.0527, Val Acc: 24.90%\nEpoch: 4, Batch: 0, Loss: 2.1121\nEpoch: 4, Batch: 100, Loss: 2.0724\nEpoch: 4, Train Loss: 2.1227, Train Acc: 23.08%, Val Loss: 2.0617, Val Acc: 26.35%\nEpoch: 5, Batch: 0, Loss: 2.1392\nEpoch: 5, Batch: 100, Loss: 2.0750\nEpoch: 5, Train Loss: 2.1218, Train Acc: 22.94%, Val Loss: 2.0504, Val Acc: 26.05%\nEpoch: 6, Batch: 0, Loss: 2.1502\nEpoch: 6, Batch: 100, Loss: 2.3083\nEpoch: 6, Train Loss: 2.1080, Train Acc: 23.00%, Val Loss: 2.0443, Val Acc: 27.45%\nEpoch: 7, Batch: 0, Loss: 2.2103\nEpoch: 7, Batch: 100, Loss: 2.1054\nEpoch: 7, Train Loss: 2.0993, Train Acc: 23.94%, Val Loss: 1.9978, Val Acc: 28.05%\nEpoch: 8, Batch: 0, Loss: 2.2541\nEpoch: 8, Batch: 100, Loss: 2.2335\nEpoch: 8, Train Loss: 2.0906, Train Acc: 24.62%, Val Loss: 2.0112, Val Acc: 27.55%\nEpoch: 9, Batch: 0, Loss: 2.1916\nEpoch: 9, Batch: 100, Loss: 1.9470\nEpoch: 9, Train Loss: 2.0832, Train Acc: 24.83%, Val Loss: 1.9837, Val Acc: 29.40%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▄▅▆▇▆▇▇██</td></tr><tr><td>train_loss</td><td>█▄▄▃▂▂▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▂▅▄▅▅▆▇▆█</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▄▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>24.8281</td></tr><tr><td>train_loss</td><td>2.08322</td></tr><tr><td>val_acc</td><td>29.4</td></tr><tr><td>val_loss</td><td>1.9837</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">clear-sweep-6</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/fu33zw1k' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/fu33zw1k</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_141341-fu33zw1k/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xv5yoe8t with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0008758675247771263\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_142326-xv5yoe8t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xv5yoe8t' target=\"_blank\">super-sweep-7</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xv5yoe8t' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xv5yoe8t</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 98, in train\n    loss.backward()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.60 GiB is free. Process 2828 has 12.14 GiB memory in use. Of the allocated memory 8.53 GiB is allocated by PyTorch, and 3.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">super-sweep-7</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xv5yoe8t' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xv5yoe8t</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_142326-xv5yoe8t/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run xv5yoe8t errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 98, in train\n    loss.backward()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.60 GiB is free. Process 2828 has 12.14 GiB memory in use. Of the allocated memory 8.53 GiB is allocated by PyTorch, and 3.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run xv5yoe8t errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 98, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss.backward()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     torch.autograd.backward(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     _engine_run_backward(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.60 GiB is free. Process 2828 has 12.14 GiB memory in use. Of the allocated memory 8.53 GiB is allocated by PyTorch, and 3.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m3uxoxiv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.008226276943111005\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_142346-m3uxoxiv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/m3uxoxiv' target=\"_blank\">rich-sweep-8</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/m3uxoxiv' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/m3uxoxiv</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 0, Loss: 2.3035\nEpoch: 0, Batch: 100, Loss: 2.3306\nEpoch: 0, Batch: 200, Loss: 2.3035\nEpoch: 0, Train Loss: 9.9989, Train Acc: 9.83%, Val Loss: 2.3048, Val Acc: 10.00%\nEpoch: 1, Batch: 0, Loss: 2.3108\nEpoch: 1, Batch: 100, Loss: 2.3073\nEpoch: 1, Batch: 200, Loss: 2.2954\nEpoch: 1, Train Loss: 14.8344, Train Acc: 9.71%, Val Loss: 2.3069, Val Acc: 10.00%\nEpoch: 2, Batch: 0, Loss: 2.3190\nEpoch: 2, Batch: 100, Loss: 2.3640\nEpoch: 2, Batch: 200, Loss: 2.3496\nEpoch: 2, Train Loss: 2.4058, Train Acc: 9.75%, Val Loss: 2.3098, Val Acc: 10.00%\nEpoch: 3, Batch: 0, Loss: 2.3034\nEpoch: 3, Batch: 100, Loss: 2.3080\nEpoch: 3, Batch: 200, Loss: 2.3026\nEpoch: 3, Train Loss: 174.9454, Train Acc: 10.34%, Val Loss: 2.3188, Val Acc: 10.00%\nEpoch: 4, Batch: 0, Loss: 2.2879\nEpoch: 4, Batch: 100, Loss: 2.3298\nEpoch: 4, Batch: 200, Loss: 2.2564\nEpoch: 4, Train Loss: 32331.1048, Train Acc: 9.38%, Val Loss: 2.3172, Val Acc: 10.00%\nEpoch: 5, Batch: 0, Loss: 2.3411\nEpoch: 5, Batch: 100, Loss: 2.3678\nEpoch: 5, Batch: 200, Loss: 2.2763\nEpoch: 5, Train Loss: 859.0246, Train Acc: 10.54%, Val Loss: 2.3358, Val Acc: 10.00%\nEpoch: 6, Batch: 0, Loss: 2.3283\nEpoch: 6, Batch: 100, Loss: 2.3269\nEpoch: 6, Batch: 200, Loss: 2.3685\nEpoch: 6, Train Loss: 2.3365, Train Acc: 9.66%, Val Loss: 2.3124, Val Acc: 10.00%\nEpoch: 7, Batch: 0, Loss: 2.2730\nEpoch: 7, Batch: 100, Loss: 2.3413\nEpoch: 7, Batch: 200, Loss: 2.3547\nEpoch: 7, Train Loss: 11113.1424, Train Acc: 9.85%, Val Loss: 2.3179, Val Acc: 10.00%\nEpoch: 8, Batch: 0, Loss: 2.3214\nEpoch: 8, Batch: 100, Loss: 2.3189\nEpoch: 8, Batch: 200, Loss: 2.3620\nEpoch: 8, Train Loss: 132352.0291, Train Acc: 10.34%, Val Loss: 2.3195, Val Acc: 10.00%\nEpoch: 9, Batch: 0, Loss: 2.2300\nEpoch: 9, Batch: 100, Loss: 2.3577\nEpoch: 9, Batch: 200, Loss: 2.2826\nEpoch: 9, Train Loss: 571.2967, Train Acc: 10.23%, Val Loss: 2.3274, Val Acc: 10.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▄▃▃▇▁█▃▄▇▆</td></tr><tr><td>train_loss</td><td>▁▁▁▁▃▁▁▂█▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▁▂▄▄█▃▄▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>10.22628</td></tr><tr><td>train_loss</td><td>571.29667</td></tr><tr><td>val_acc</td><td>10</td></tr><tr><td>val_loss</td><td>2.32744</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">rich-sweep-8</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/m3uxoxiv' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/m3uxoxiv</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_142346-m3uxoxiv/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cidy1tdb with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.004130726350273492\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_143321-cidy1tdb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/cidy1tdb' target=\"_blank\">sleek-sweep-9</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/cidy1tdb' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/cidy1tdb</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 0, Loss: 2.2583\nEpoch: 0, Batch: 100, Loss: 2.1995\nEpoch: 0, Batch: 200, Loss: 2.3984\nEpoch: 0, Train Loss: 2.3797, Train Acc: 14.34%, Val Loss: 2.2301, Val Acc: 16.80%\nEpoch: 1, Batch: 0, Loss: 2.2545\nEpoch: 1, Batch: 100, Loss: 2.2533\nEpoch: 1, Batch: 200, Loss: 2.2480\nEpoch: 1, Train Loss: 2.2725, Train Acc: 14.53%, Val Loss: 2.2405, Val Acc: 16.30%\nEpoch: 2, Batch: 0, Loss: 2.2700\nEpoch: 2, Batch: 100, Loss: 2.1041\nEpoch: 2, Batch: 200, Loss: 2.2587\nEpoch: 2, Train Loss: 2.2805, Train Acc: 15.10%, Val Loss: 2.2325, Val Acc: 18.25%\nEpoch: 3, Batch: 0, Loss: 2.2997\nEpoch: 3, Batch: 100, Loss: 2.1544\nEpoch: 3, Batch: 200, Loss: 2.2006\nEpoch: 3, Train Loss: 2.2692, Train Acc: 14.89%, Val Loss: 2.2333, Val Acc: 17.70%\nEpoch: 4, Batch: 0, Loss: 2.2668\nEpoch: 4, Batch: 100, Loss: 2.3135\nEpoch: 4, Batch: 200, Loss: 2.2268\nEpoch: 4, Train Loss: 2.2599, Train Acc: 16.04%, Val Loss: 2.1838, Val Acc: 19.65%\nEpoch: 5, Batch: 0, Loss: 2.2601\nEpoch: 5, Batch: 100, Loss: 2.2797\nEpoch: 5, Batch: 200, Loss: 2.2197\nEpoch: 5, Train Loss: 2.2509, Train Acc: 15.71%, Val Loss: 2.1969, Val Acc: 18.30%\nEpoch: 6, Batch: 0, Loss: 2.1659\nEpoch: 6, Batch: 100, Loss: 2.2796\nEpoch: 6, Batch: 200, Loss: 2.2230\nEpoch: 6, Train Loss: 2.2343, Train Acc: 16.78%, Val Loss: 2.1908, Val Acc: 19.30%\nEpoch: 7, Batch: 0, Loss: 2.1057\nEpoch: 7, Batch: 100, Loss: 2.2247\nEpoch: 7, Batch: 200, Loss: 2.2795\nEpoch: 7, Train Loss: 2.2257, Train Acc: 17.19%, Val Loss: 2.1767, Val Acc: 19.95%\nEpoch: 8, Batch: 0, Loss: 2.2643\nEpoch: 8, Batch: 100, Loss: 2.3519\nEpoch: 8, Batch: 200, Loss: 2.1687\nEpoch: 8, Train Loss: 2.2158, Train Acc: 18.04%, Val Loss: 2.1545, Val Acc: 21.40%\nEpoch: 9, Batch: 0, Loss: 2.3318\nEpoch: 9, Batch: 100, Loss: 2.2855\nEpoch: 9, Batch: 200, Loss: 2.0887\nEpoch: 9, Train Loss: 2.2113, Train Acc: 18.44%, Val Loss: 2.1543, Val Acc: 22.45%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▁▂▂▄▃▅▆▇█</td></tr><tr><td>train_loss</td><td>█▄▄▃▃▃▂▂▁▁</td></tr><tr><td>val_acc</td><td>▂▁▃▃▅▃▄▅▇█</td></tr><tr><td>val_loss</td><td>▇█▇▇▃▄▄▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>18.4398</td></tr><tr><td>train_loss</td><td>2.21129</td></tr><tr><td>val_acc</td><td>22.45</td></tr><tr><td>val_loss</td><td>2.15431</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sleek-sweep-9</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/cidy1tdb' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/cidy1tdb</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_143321-cidy1tdb/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dgbr96fh with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0012966664538758648\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_144321-dgbr96fh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/dgbr96fh' target=\"_blank\">eternal-sweep-10</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/dgbr96fh' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/dgbr96fh</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 0, Loss: 2.3179\nEpoch: 0, Batch: 100, Loss: 2.1643\nEpoch: 0, Train Loss: 2.2217, Train Acc: 18.60%, Val Loss: 2.1017, Val Acc: 22.65%\nEpoch: 1, Batch: 0, Loss: 2.1726\nEpoch: 1, Batch: 100, Loss: 2.1358\nEpoch: 1, Train Loss: 2.1306, Train Acc: 22.40%, Val Loss: 2.0505, Val Acc: 26.40%\nEpoch: 2, Batch: 0, Loss: 1.9505\nEpoch: 2, Batch: 100, Loss: 2.1281\nEpoch: 2, Train Loss: 2.0895, Train Acc: 24.90%, Val Loss: 2.0364, Val Acc: 27.30%\nEpoch: 3, Batch: 0, Loss: 2.1432\nEpoch: 3, Batch: 100, Loss: 1.8350\nEpoch: 3, Train Loss: 2.0575, Train Acc: 26.13%, Val Loss: 1.9955, Val Acc: 28.50%\nEpoch: 4, Batch: 0, Loss: 2.1234\nEpoch: 4, Batch: 100, Loss: 1.8677\nEpoch: 4, Train Loss: 2.0448, Train Acc: 27.13%, Val Loss: 1.9993, Val Acc: 27.95%\nEpoch: 5, Batch: 0, Loss: 2.0852\nEpoch: 5, Batch: 100, Loss: 1.9188\nEpoch: 5, Train Loss: 2.0178, Train Acc: 27.67%, Val Loss: 1.9334, Val Acc: 32.05%\nEpoch: 6, Batch: 0, Loss: 2.2200\nEpoch: 6, Batch: 100, Loss: 2.0234\nEpoch: 6, Train Loss: 1.9964, Train Acc: 28.67%, Val Loss: 1.9515, Val Acc: 29.50%\nEpoch: 7, Batch: 0, Loss: 1.9675\nEpoch: 7, Batch: 100, Loss: 1.8102\nEpoch: 7, Train Loss: 1.9779, Train Acc: 29.23%, Val Loss: 1.9259, Val Acc: 31.25%\nEpoch: 8, Batch: 0, Loss: 1.9181\nEpoch: 8, Batch: 100, Loss: 1.9516\nEpoch: 8, Train Loss: 1.9723, Train Acc: 30.23%, Val Loss: 1.9365, Val Acc: 31.45%\nEpoch: 9, Batch: 0, Loss: 1.8471\nEpoch: 9, Batch: 100, Loss: 1.9980\nEpoch: 9, Train Loss: 1.9605, Train Acc: 30.45%, Val Loss: 1.8975, Val Acc: 34.05%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▃▅▅▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▃▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▄▅▄▇▅▆▆█</td></tr><tr><td>val_loss</td><td>█▆▆▄▄▂▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>30.45381</td></tr><tr><td>train_loss</td><td>1.96051</td></tr><tr><td>val_acc</td><td>34.05</td></tr><tr><td>val_loss</td><td>1.89752</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">eternal-sweep-10</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/dgbr96fh' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/dgbr96fh</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_144321-dgbr96fh/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v8al6n6b with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005976550881162094\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_145321-v8al6n6b</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/v8al6n6b' target=\"_blank\">peach-sweep-11</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/v8al6n6b' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/v8al6n6b</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 0, Loss: 2.3500\nEpoch: 0, Batch: 100, Loss: 2.4284\nEpoch: 0, Batch: 200, Loss: 2.3046\nEpoch: 0, Train Loss: 3.1598, Train Acc: 10.84%, Val Loss: 2.3013, Val Acc: 10.35%\nEpoch: 1, Batch: 0, Loss: 2.2850\nEpoch: 1, Batch: 100, Loss: 2.2883\nEpoch: 1, Batch: 200, Loss: 2.3316\nEpoch: 1, Train Loss: 2.2998, Train Acc: 11.61%, Val Loss: 2.2885, Val Acc: 12.90%\nEpoch: 2, Batch: 0, Loss: 2.3292\nEpoch: 2, Batch: 100, Loss: 2.4797\nEpoch: 2, Batch: 200, Loss: 2.3010\nEpoch: 2, Train Loss: 2.3000, Train Acc: 10.61%, Val Loss: 2.3029, Val Acc: 10.00%\nEpoch: 3, Batch: 0, Loss: 2.2926\nEpoch: 3, Batch: 100, Loss: 2.3002\nEpoch: 3, Batch: 200, Loss: 2.2661\nEpoch: 3, Train Loss: 2.2963, Train Acc: 10.33%, Val Loss: 2.2735, Val Acc: 13.55%\nEpoch: 4, Batch: 0, Loss: 2.3862\nEpoch: 4, Batch: 100, Loss: 2.2711\nEpoch: 4, Batch: 200, Loss: 2.3203\nEpoch: 4, Train Loss: 2.2829, Train Acc: 12.14%, Val Loss: 2.2620, Val Acc: 14.05%\nEpoch: 5, Batch: 0, Loss: 2.2662\nEpoch: 5, Batch: 100, Loss: 2.3130\nEpoch: 5, Batch: 200, Loss: 2.2837\nEpoch: 5, Train Loss: 2.2775, Train Acc: 12.55%, Val Loss: 2.2561, Val Acc: 14.95%\nEpoch: 6, Batch: 0, Loss: 2.2557\nEpoch: 6, Batch: 100, Loss: 2.3667\nEpoch: 6, Batch: 200, Loss: 2.3001\nEpoch: 6, Train Loss: 2.2729, Train Acc: 13.31%, Val Loss: 2.2410, Val Acc: 14.45%\nEpoch: 7, Batch: 0, Loss: 2.3395\nEpoch: 7, Batch: 100, Loss: 2.1675\nEpoch: 7, Batch: 200, Loss: 2.2244\nEpoch: 7, Train Loss: 2.2625, Train Acc: 13.88%, Val Loss: 2.2587, Val Acc: 14.30%\nEpoch: 8, Batch: 0, Loss: 2.2512\nEpoch: 8, Batch: 100, Loss: 2.2045\nEpoch: 8, Batch: 200, Loss: 2.2743\nEpoch: 8, Train Loss: 2.2549, Train Acc: 14.01%, Val Loss: 2.2396, Val Acc: 15.15%\nEpoch: 9, Batch: 0, Loss: 2.2452\nEpoch: 9, Batch: 100, Loss: 2.3652\nEpoch: 9, Batch: 200, Loss: 2.2618\nEpoch: 9, Train Loss: 2.2552, Train Acc: 14.46%, Val Loss: 2.2349, Val Acc: 15.70%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▂▃▁▁▄▅▆▇▇█</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▁▅▆▇▆▆▇█</td></tr><tr><td>val_loss</td><td>█▇█▅▄▃▂▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>14.46431</td></tr><tr><td>train_loss</td><td>2.2552</td></tr><tr><td>val_acc</td><td>15.7</td></tr><tr><td>val_loss</td><td>2.23489</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">peach-sweep-11</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/v8al6n6b' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/v8al6n6b</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_145321-v8al6n6b/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jsn80rj0 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.004635447500400798\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_150702-jsn80rj0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/jsn80rj0' target=\"_blank\">wild-sweep-12</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/jsn80rj0' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/jsn80rj0</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 0, Loss: 2.3060\nEpoch: 0, Train Loss: 2.4310, Train Acc: 10.49%, Val Loss: 2.3037, Val Acc: 10.00%\nEpoch: 1, Batch: 0, Loss: 2.2981\nEpoch: 1, Train Loss: 2.3354, Train Acc: 10.20%, Val Loss: 2.3028, Val Acc: 10.00%\nEpoch: 2, Batch: 0, Loss: 2.4152\nEpoch: 2, Train Loss: 2.3732, Train Acc: 9.96%, Val Loss: 2.3027, Val Acc: 10.00%\nEpoch: 3, Batch: 0, Loss: 2.3687\nEpoch: 3, Train Loss: 2.5486, Train Acc: 9.86%, Val Loss: 2.3027, Val Acc: 10.00%\nEpoch: 4, Batch: 0, Loss: 2.3524\nEpoch: 4, Train Loss: 2.6887, Train Acc: 9.93%, Val Loss: 2.3028, Val Acc: 10.00%\nEpoch: 5, Batch: 0, Loss: 2.4322\nEpoch: 5, Train Loss: 4.5069, Train Acc: 10.16%, Val Loss: 2.3033, Val Acc: 10.00%\nEpoch: 6, Batch: 0, Loss: 2.5741\nEpoch: 6, Train Loss: 3.0591, Train Acc: 9.53%, Val Loss: 2.3030, Val Acc: 10.00%\nEpoch: 7, Batch: 0, Loss: 2.4415\nEpoch: 7, Train Loss: 2.3211, Train Acc: 9.48%, Val Loss: 2.3026, Val Acc: 10.00%\nEpoch: 8, Batch: 0, Loss: 2.5338\nEpoch: 8, Train Loss: 2.3383, Train Acc: 9.59%, Val Loss: 2.3026, Val Acc: 10.00%\nEpoch: 9, Batch: 0, Loss: 2.3058\nEpoch: 9, Train Loss: 2.3279, Train Acc: 9.86%, Val Loss: 2.3028, Val Acc: 10.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>█▆▄▄▄▆▁▁▂▄</td></tr><tr><td>train_loss</td><td>▁▁▁▂▂█▃▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▁▂▂▅▃▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>9.86373</td></tr><tr><td>train_loss</td><td>2.32794</td></tr><tr><td>val_acc</td><td>10</td></tr><tr><td>val_loss</td><td>2.30285</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">wild-sweep-12</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/jsn80rj0' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/jsn80rj0</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_150702-jsn80rj0/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ntb60cxe with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.004517723633492921\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_151616-ntb60cxe</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ntb60cxe' target=\"_blank\">autumn-sweep-13</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ntb60cxe' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ntb60cxe</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 0, Loss: 2.3010\nEpoch: 0, Batch: 100, Loss: 2.3043\nEpoch: 0, Batch: 200, Loss: 2.3027\nEpoch: 0, Train Loss: 2.3142, Train Acc: 10.01%, Val Loss: 2.3036, Val Acc: 10.00%\nEpoch: 1, Batch: 0, Loss: 2.2953\nEpoch: 1, Batch: 100, Loss: 2.2960\nEpoch: 1, Batch: 200, Loss: 2.3061\nEpoch: 1, Train Loss: 2.3037, Train Acc: 9.76%, Val Loss: 2.3028, Val Acc: 10.00%\nEpoch: 2, Batch: 0, Loss: 2.3017\nEpoch: 2, Batch: 100, Loss: 2.2972\nEpoch: 2, Batch: 200, Loss: 2.3051\nEpoch: 2, Train Loss: 2.3036, Train Acc: 9.19%, Val Loss: 2.3027, Val Acc: 10.00%\nEpoch: 3, Batch: 0, Loss: 2.3028\nEpoch: 3, Batch: 100, Loss: 2.3123\nEpoch: 3, Batch: 200, Loss: 2.3040\nEpoch: 3, Train Loss: 2.3036, Train Acc: 9.31%, Val Loss: 2.3028, Val Acc: 10.00%\nEpoch: 4, Batch: 0, Loss: 2.2977\nEpoch: 4, Batch: 100, Loss: 2.2977\nEpoch: 4, Batch: 200, Loss: 2.3038\nEpoch: 4, Train Loss: 2.3034, Train Acc: 9.76%, Val Loss: 2.3029, Val Acc: 10.00%\nEpoch: 5, Batch: 0, Loss: 2.2971\nEpoch: 5, Batch: 100, Loss: 2.3152\nEpoch: 5, Batch: 200, Loss: 2.3004\nEpoch: 5, Train Loss: 2.3035, Train Acc: 9.43%, Val Loss: 2.3027, Val Acc: 10.00%\nEpoch: 6, Batch: 0, Loss: 2.3016\nEpoch: 6, Batch: 100, Loss: 2.3099\nEpoch: 6, Batch: 200, Loss: 2.3006\nEpoch: 6, Train Loss: 2.3035, Train Acc: 9.56%, Val Loss: 2.3027, Val Acc: 10.00%\nEpoch: 7, Batch: 0, Loss: 2.3041\nEpoch: 7, Batch: 100, Loss: 2.3036\nEpoch: 7, Batch: 200, Loss: 2.2994\nEpoch: 7, Train Loss: 2.3036, Train Acc: 9.29%, Val Loss: 2.3026, Val Acc: 10.00%\nEpoch: 8, Batch: 0, Loss: 2.3064\nEpoch: 8, Batch: 100, Loss: 2.3001\nEpoch: 8, Batch: 200, Loss: 2.3072\nEpoch: 8, Train Loss: 2.3036, Train Acc: 9.43%, Val Loss: 2.3027, Val Acc: 10.00%\nEpoch: 9, Batch: 0, Loss: 2.3035\nEpoch: 9, Batch: 100, Loss: 2.3014\nEpoch: 9, Batch: 200, Loss: 2.2941\nEpoch: 9, Train Loss: 2.3037, Train Acc: 9.54%, Val Loss: 2.3027, Val Acc: 10.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>█▆▁▂▆▃▄▂▃▄</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▁▂▃▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>9.53869</td></tr><tr><td>train_loss</td><td>2.30365</td></tr><tr><td>val_acc</td><td>10</td></tr><tr><td>val_loss</td><td>2.30275</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">autumn-sweep-13</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ntb60cxe' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ntb60cxe</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_151616-ntb60cxe/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xg5mkpt7 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00242225743608726\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_152514-xg5mkpt7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xg5mkpt7' target=\"_blank\">woven-sweep-14</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xg5mkpt7' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xg5mkpt7</a>"},"metadata":{}},{"name":"stdout","text":"Epoch: 0, Batch: 0, Loss: 2.3455\nEpoch: 0, Batch: 100, Loss: 2.3301\nEpoch: 0, Batch: 200, Loss: 2.2954\nEpoch: 0, Train Loss: 2.3187, Train Acc: 16.29%, Val Loss: 2.1646, Val Acc: 20.85%\nEpoch: 1, Batch: 0, Loss: 2.1849\nEpoch: 1, Batch: 100, Loss: 2.2927\nEpoch: 1, Batch: 200, Loss: 1.9726\nEpoch: 1, Train Loss: 2.1999, Train Acc: 19.05%, Val Loss: 2.1467, Val Acc: 21.40%\nEpoch: 2, Batch: 0, Loss: 2.1648\nEpoch: 2, Batch: 100, Loss: 2.1491\nEpoch: 2, Batch: 200, Loss: 2.2973\nEpoch: 2, Train Loss: 2.1796, Train Acc: 20.55%, Val Loss: 2.1316, Val Acc: 24.45%\nEpoch: 3, Batch: 0, Loss: 2.2124\nEpoch: 3, Batch: 100, Loss: 2.2534\nEpoch: 3, Batch: 200, Loss: 2.2850\nEpoch: 3, Train Loss: 2.1771, Train Acc: 20.47%, Val Loss: 2.0971, Val Acc: 25.00%\nEpoch: 4, Batch: 0, Loss: 2.1871\nEpoch: 4, Batch: 100, Loss: 2.2721\nEpoch: 4, Batch: 200, Loss: 2.2759\nEpoch: 4, Train Loss: 2.1691, Train Acc: 20.63%, Val Loss: 2.1029, Val Acc: 23.60%\nEpoch: 5, Batch: 0, Loss: 2.1734\nEpoch: 5, Batch: 100, Loss: 2.2390\nEpoch: 5, Batch: 200, Loss: 1.9620\nEpoch: 5, Train Loss: 2.1466, Train Acc: 21.33%, Val Loss: 2.0535, Val Acc: 25.20%\nEpoch: 6, Batch: 0, Loss: 2.1771\nEpoch: 6, Batch: 100, Loss: 2.1103\nEpoch: 6, Batch: 200, Loss: 2.0271\nEpoch: 6, Train Loss: 2.1268, Train Acc: 22.60%, Val Loss: 2.0790, Val Acc: 25.25%\nEpoch: 7, Batch: 0, Loss: 2.1713\nEpoch: 7, Batch: 100, Loss: 2.2510\nEpoch: 7, Batch: 200, Loss: 2.4310\nEpoch: 7, Train Loss: 2.1166, Train Acc: 23.45%, Val Loss: 2.0640, Val Acc: 25.40%\nEpoch: 8, Batch: 0, Loss: 2.1672\nEpoch: 8, Batch: 100, Loss: 1.9574\nEpoch: 8, Batch: 200, Loss: 1.8841\nEpoch: 8, Train Loss: 2.1108, Train Acc: 23.50%, Val Loss: 2.0666, Val Acc: 26.60%\nEpoch: 9, Batch: 0, Loss: 2.0632\nEpoch: 9, Batch: 100, Loss: 2.2684\nEpoch: 9, Batch: 200, Loss: 2.0018\nEpoch: 9, Train Loss: 2.0958, Train Acc: 24.03%, Val Loss: 2.0597, Val Acc: 25.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_acc</td><td>▁▃▅▅▅▆▇▇██</td></tr><tr><td>train_loss</td><td>█▄▄▄▃▃▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▂▅▆▄▆▆▇█▆</td></tr><tr><td>val_loss</td><td>█▇▆▄▄▁▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_acc</td><td>24.028</td></tr><tr><td>train_loss</td><td>2.09584</td></tr><tr><td>val_acc</td><td>25</td></tr><tr><td>val_loss</td><td>2.0597</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">woven-sweep-14</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xg5mkpt7' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xg5mkpt7</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_152514-xg5mkpt7/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nvv45grv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.003021100793910555\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153455-nvv45grv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/nvv45grv' target=\"_blank\">wandering-sweep-15</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/nvv45grv' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/nvv45grv</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.60 GiB is free. Process 2828 has 12.14 GiB memory in use. Of the allocated memory 7.84 GiB is allocated by PyTorch, and 4.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">wandering-sweep-15</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/nvv45grv' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/nvv45grv</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153455-nvv45grv/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run nvv45grv errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.60 GiB is free. Process 2828 has 12.14 GiB memory in use. Of the allocated memory 7.84 GiB is allocated by PyTorch, and 4.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run nvv45grv errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.60 GiB is free. Process 2828 has 12.14 GiB memory in use. Of the allocated memory 7.84 GiB is allocated by PyTorch, and 4.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: klipmght with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0007694616390209478\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153515-klipmght</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/klipmght' target=\"_blank\">earthy-sweep-16</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/klipmght' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/klipmght</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\", line 471, in forward\n    return F.mish(input, inplace=self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2400, in mish\n    return torch._C._nn.mish(input)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.06 GiB is free. Process 2828 has 13.67 GiB memory in use. Of the allocated memory 10.96 GiB is allocated by PyTorch, and 2.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">earthy-sweep-16</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/klipmght' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/klipmght</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153515-klipmght/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run klipmght errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\", line 471, in forward\n    return F.mish(input, inplace=self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2400, in mish\n    return torch._C._nn.mish(input)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.06 GiB is free. Process 2828 has 13.67 GiB memory in use. Of the allocated memory 10.96 GiB is allocated by PyTorch, and 2.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run klipmght errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\", line 471, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.mish(input, inplace=self.inplace)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2400, in mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch._C._nn.mish(input)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.06 GiB is free. Process 2828 has 13.67 GiB memory in use. Of the allocated memory 10.96 GiB is allocated by PyTorch, and 2.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fkpd784l with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0033618481199117335\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153536-fkpd784l</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/fkpd784l' target=\"_blank\">mild-sweep-17</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/fkpd784l' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/fkpd784l</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 98, in train\n    loss.backward()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 306.12 MiB is free. Process 2828 has 14.44 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">mild-sweep-17</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/fkpd784l' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/fkpd784l</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153536-fkpd784l/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run fkpd784l errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 98, in train\n    loss.backward()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 306.12 MiB is free. Process 2828 has 14.44 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run fkpd784l errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 98, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss.backward()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     torch.autograd.backward(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     _engine_run_backward(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 306.12 MiB is free. Process 2828 has 14.44 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: aiqsna3s with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.006525549954260273\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153556-aiqsna3s</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/aiqsna3s' target=\"_blank\">stellar-sweep-18</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/aiqsna3s' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/aiqsna3s</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 306.12 MiB is free. Process 2828 has 14.44 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">stellar-sweep-18</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/aiqsna3s' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/aiqsna3s</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153556-aiqsna3s/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run aiqsna3s errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 306.12 MiB is free. Process 2828 has 14.44 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run aiqsna3s errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 306.12 MiB is free. Process 2828 has 14.44 GiB memory in use. Of the allocated memory 13.02 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4wnmisss with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.008753864164027314\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153617-4wnmisss</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/4wnmisss' target=\"_blank\">fearless-sweep-19</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/4wnmisss' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/4wnmisss</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 304.12 MiB is free. Process 2828 has 14.44 GiB memory in use. Of the allocated memory 13.06 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fearless-sweep-19</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/4wnmisss' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/4wnmisss</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153617-4wnmisss/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 4wnmisss errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 304.12 MiB is free. Process 2828 has 14.44 GiB memory in use. Of the allocated memory 13.06 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4wnmisss errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 304.12 MiB is free. Process 2828 has 14.44 GiB memory in use. Of the allocated memory 13.06 GiB is allocated by PyTorch, and 1.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 42fu4yb7 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.006764566618615511\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153632-42fu4yb7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/42fu4yb7' target=\"_blank\">apricot-sweep-20</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/42fu4yb7' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/42fu4yb7</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 300.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.10 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">apricot-sweep-20</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/42fu4yb7' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/42fu4yb7</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153632-42fu4yb7/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 42fu4yb7 errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 300.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.10 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 42fu4yb7 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 300.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.10 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l4yq0gwt with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.004215704537411238\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153658-l4yq0gwt</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/l4yq0gwt' target=\"_blank\">royal-sweep-21</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/l4yq0gwt' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/l4yq0gwt</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 298.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">royal-sweep-21</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/l4yq0gwt' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/l4yq0gwt</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153658-l4yq0gwt/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run l4yq0gwt errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 298.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run l4yq0gwt errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 298.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.18 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5ylt8iy8 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.003668565063325221\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153718-5ylt8iy8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5ylt8iy8' target=\"_blank\">smooth-sweep-22</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5ylt8iy8' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5ylt8iy8</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 296.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.22 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">smooth-sweep-22</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5ylt8iy8' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5ylt8iy8</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153718-5ylt8iy8/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 5ylt8iy8 errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 296.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.22 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5ylt8iy8 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 296.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.22 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oo7hz128 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001230777323589442\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153739-oo7hz128</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/oo7hz128' target=\"_blank\">dandy-sweep-23</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/oo7hz128' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/oo7hz128</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 294.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dandy-sweep-23</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/oo7hz128' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/oo7hz128</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153739-oo7hz128/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run oo7hz128 errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 294.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run oo7hz128 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 294.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.27 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oknmfb6j with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009593414242589534\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153759-oknmfb6j</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/oknmfb6j' target=\"_blank\">serene-sweep-24</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/oknmfb6j' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/oknmfb6j</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 294.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 1017.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">serene-sweep-24</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/oknmfb6j' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/oknmfb6j</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153759-oknmfb6j/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run oknmfb6j errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 294.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 1017.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run oknmfb6j errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 294.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, and 1017.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 877hvm1b with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0007489405312372015\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153824-877hvm1b</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/877hvm1b' target=\"_blank\">fine-sweep-25</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/877hvm1b' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/877hvm1b</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 294.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.41 GiB is allocated by PyTorch, and 925.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fine-sweep-25</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/877hvm1b' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/877hvm1b</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153824-877hvm1b/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 877hvm1b errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 294.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.41 GiB is allocated by PyTorch, and 925.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 877hvm1b errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 294.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.41 GiB is allocated by PyTorch, and 925.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: isfgo9ke with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0012293866945038555\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153845-isfgo9ke</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/isfgo9ke' target=\"_blank\">lemon-sweep-26</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/isfgo9ke' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/isfgo9ke</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 294.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.45 GiB is allocated by PyTorch, and 880.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lemon-sweep-26</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/isfgo9ke' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/isfgo9ke</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153845-isfgo9ke/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run isfgo9ke errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 294.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.45 GiB is allocated by PyTorch, and 880.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run isfgo9ke errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 294.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.45 GiB is allocated by PyTorch, and 880.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: coqedy8t with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0024547754002303495\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153911-coqedy8t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/coqedy8t' target=\"_blank\">lilac-sweep-27</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/coqedy8t' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/coqedy8t</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 292.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.53 GiB is allocated by PyTorch, and 801.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lilac-sweep-27</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/coqedy8t' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/coqedy8t</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153911-coqedy8t/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run coqedy8t errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 292.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.53 GiB is allocated by PyTorch, and 801.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run coqedy8t errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 292.12 MiB is free. Process 2828 has 14.45 GiB memory in use. Of the allocated memory 13.53 GiB is allocated by PyTorch, and 801.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: khduzg0o with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00042598655101635575\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153931-khduzg0o</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/khduzg0o' target=\"_blank\">honest-sweep-28</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/khduzg0o' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/khduzg0o</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n    return F.batch_norm(\n           ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2812, in batch_norm\n    return torch.batch_norm(\n           ^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 290.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 385.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">honest-sweep-28</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/khduzg0o' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/khduzg0o</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153931-khduzg0o/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run khduzg0o errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n    return F.batch_norm(\n           ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2812, in batch_norm\n    return torch.batch_norm(\n           ^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 290.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 385.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run khduzg0o errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\", line 193, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.batch_norm(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2812, in batch_norm\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.batch_norm(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 290.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 385.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5uaielsr with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0022389869246512324\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_153947-5uaielsr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5uaielsr' target=\"_blank\">whole-sweep-29</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5uaielsr' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5uaielsr</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 288.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 342.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">whole-sweep-29</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5uaielsr' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5uaielsr</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_153947-5uaielsr/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 5uaielsr errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 288.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 342.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5uaielsr errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 288.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 342.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ct70af3n with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0007809239897641843\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154007-ct70af3n</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ct70af3n' target=\"_blank\">revived-sweep-30</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ct70af3n' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ct70af3n</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 286.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 300.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">revived-sweep-30</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ct70af3n' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ct70af3n</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154007-ct70af3n/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run ct70af3n errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 286.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 300.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ct70af3n errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 286.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 300.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g03eivnw with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001083295573354948\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154028-g03eivnw</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/g03eivnw' target=\"_blank\">tough-sweep-31</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/g03eivnw' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/g03eivnw</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 286.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 222.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">tough-sweep-31</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/g03eivnw' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/g03eivnw</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154028-g03eivnw/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run g03eivnw errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 286.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 222.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run g03eivnw errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 286.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 222.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hitxc31d with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0027281764924946463\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154048-hitxc31d</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/hitxc31d' target=\"_blank\">colorful-sweep-32</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/hitxc31d' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/hitxc31d</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 286.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 136.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">colorful-sweep-32</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/hitxc31d' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/hitxc31d</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154048-hitxc31d/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run hitxc31d errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 286.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 136.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run hitxc31d errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 286.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 136.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5jlm17w5 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0011308720756114516\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154109-5jlm17w5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5jlm17w5' target=\"_blank\">rose-sweep-33</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5jlm17w5' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5jlm17w5</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 286.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 86.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">rose-sweep-33</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5jlm17w5' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5jlm17w5</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154109-5jlm17w5/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 5jlm17w5 errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 286.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 86.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5jlm17w5 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 286.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 86.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r036baa7 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.002434117528293314\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154130-r036baa7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/r036baa7' target=\"_blank\">earnest-sweep-34</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/r036baa7' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/r036baa7</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 284.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.31 GiB is allocated by PyTorch, and 10.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">earnest-sweep-34</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/r036baa7' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/r036baa7</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154130-r036baa7/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run r036baa7 errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 284.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.31 GiB is allocated by PyTorch, and 10.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run r036baa7 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 284.12 MiB is free. Process 2828 has 14.46 GiB memory in use. Of the allocated memory 14.31 GiB is allocated by PyTorch, and 10.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c8i5yxpn with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0014598425148240675\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154150-c8i5yxpn</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c8i5yxpn' target=\"_blank\">blooming-sweep-35</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c8i5yxpn' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c8i5yxpn</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 226.12 MiB is free. Process 2828 has 14.52 GiB memory in use. Of the allocated memory 14.36 GiB is allocated by PyTorch, and 18.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">blooming-sweep-35</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c8i5yxpn' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c8i5yxpn</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154150-c8i5yxpn/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run c8i5yxpn errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 226.12 MiB is free. Process 2828 has 14.52 GiB memory in use. Of the allocated memory 14.36 GiB is allocated by PyTorch, and 18.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run c8i5yxpn errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 226.12 MiB is free. Process 2828 has 14.52 GiB memory in use. Of the allocated memory 14.36 GiB is allocated by PyTorch, and 18.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 04gsdeuv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002905665116277379\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154211-04gsdeuv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/04gsdeuv' target=\"_blank\">atomic-sweep-36</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/04gsdeuv' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/04gsdeuv</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 150.12 MiB is free. Process 2828 has 14.59 GiB memory in use. Of the allocated memory 14.44 GiB is allocated by PyTorch, and 13.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">atomic-sweep-36</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/04gsdeuv' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/04gsdeuv</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154211-04gsdeuv/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 04gsdeuv errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 150.12 MiB is free. Process 2828 has 14.59 GiB memory in use. Of the allocated memory 14.44 GiB is allocated by PyTorch, and 13.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 04gsdeuv errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 14.74 GiB of which 150.12 MiB is free. Process 2828 has 14.59 GiB memory in use. Of the allocated memory 14.44 GiB is allocated by PyTorch, and 13.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: by1hu123 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005153455170956662\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154231-by1hu123</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/by1hu123' target=\"_blank\">bumbling-sweep-37</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/by1hu123' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/by1hu123</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 110.12 MiB is free. Process 2828 has 14.63 GiB memory in use. Of the allocated memory 14.49 GiB is allocated by PyTorch, and 8.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">bumbling-sweep-37</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/by1hu123' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/by1hu123</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154231-by1hu123/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run by1hu123 errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 110.12 MiB is free. Process 2828 has 14.63 GiB memory in use. Of the allocated memory 14.49 GiB is allocated by PyTorch, and 8.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run by1hu123 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 110.12 MiB is free. Process 2828 has 14.63 GiB memory in use. Of the allocated memory 14.49 GiB is allocated by PyTorch, and 8.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mfayxagg with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001398109126621504\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154252-mfayxagg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/mfayxagg' target=\"_blank\">honest-sweep-38</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/mfayxagg' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/mfayxagg</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 2828 has 14.72 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 16.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">honest-sweep-38</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/mfayxagg' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/mfayxagg</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154252-mfayxagg/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run mfayxagg errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n    outputs = model(inputs)\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n    x = block(x)\n        ^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 2828 has 14.72 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 16.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run mfayxagg errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 96, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     outputs = model(inputs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1310665857.py\", line 73, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = block(x)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 2828 has 14.72 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 16.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 89vgndz6 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0021774677174940726\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154313-89vgndz6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/89vgndz6' target=\"_blank\">snowy-sweep-39</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/89vgndz6' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/89vgndz6</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 93, in train\n    inputs, targets = inputs.to(device), targets.to(device)\n                      ^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 2828 has 14.72 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 13.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">snowy-sweep-39</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/89vgndz6' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/89vgndz6</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154313-89vgndz6/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 89vgndz6 errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 93, in train\n    inputs, targets = inputs.to(device), targets.to(device)\n                      ^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 2828 has 14.72 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 13.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 89vgndz6 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 93, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     inputs, targets = inputs.to(device), targets.to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                       ^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 2828 has 14.72 GiB memory in use. Of the allocated memory 14.57 GiB is allocated by PyTorch, and 13.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c7c0nzf9 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.003139072565156746\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154338-c7c0nzf9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c7c0nzf9' target=\"_blank\">fancy-sweep-40</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c7c0nzf9' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c7c0nzf9</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 13.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fancy-sweep-40</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c7c0nzf9' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c7c0nzf9</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154338-c7c0nzf9/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run c7c0nzf9 errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 13.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run c7c0nzf9 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 13.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c423p2t4 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00267254642618888\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154354-c423p2t4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c423p2t4' target=\"_blank\">mild-sweep-41</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c423p2t4' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c423p2t4</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 13.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">mild-sweep-41</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c423p2t4' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/c423p2t4</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154354-c423p2t4/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run c423p2t4 errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 13.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run c423p2t4 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 13.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5blu9esh with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0024089342609622485\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154410-5blu9esh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5blu9esh' target=\"_blank\">devout-sweep-42</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5blu9esh' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5blu9esh</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 12.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">devout-sweep-42</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5blu9esh' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/5blu9esh</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154410-5blu9esh/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 5blu9esh errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 12.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5blu9esh errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 12.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 69qcflg3 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001289083451884306\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154425-69qcflg3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/69qcflg3' target=\"_blank\">devoted-sweep-43</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/69qcflg3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/69qcflg3</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 8.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">devoted-sweep-43</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/69qcflg3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/69qcflg3</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154425-69qcflg3/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 69qcflg3 errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 8.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 69qcflg3 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 8.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4nhlohmp with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0014153531880373052\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154441-4nhlohmp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/4nhlohmp' target=\"_blank\">laced-sweep-44</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/4nhlohmp' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/4nhlohmp</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 8.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">laced-sweep-44</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/4nhlohmp' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/4nhlohmp</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154441-4nhlohmp/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 4nhlohmp errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 8.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4nhlohmp errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 8.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u8tojhin with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001198925726439609\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154456-u8tojhin</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/u8tojhin' target=\"_blank\">vocal-sweep-45</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/u8tojhin' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/u8tojhin</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 8.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vocal-sweep-45</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/u8tojhin' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/u8tojhin</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154456-u8tojhin/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run u8tojhin errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 8.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run u8tojhin errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 8.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ziszrqb2 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0038227115026480854\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154512-ziszrqb2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ziszrqb2' target=\"_blank\">sparkling-sweep-46</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ziszrqb2' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ziszrqb2</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 9.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sparkling-sweep-46</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ziszrqb2' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/ziszrqb2</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154512-ziszrqb2/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run ziszrqb2 errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 9.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ziszrqb2 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 9.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: csteat8u with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.003285511150508247\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154527-csteat8u</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/csteat8u' target=\"_blank\">desert-sweep-47</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/csteat8u' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/csteat8u</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 9.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">desert-sweep-47</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/csteat8u' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/csteat8u</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154527-csteat8u/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run csteat8u errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 9.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run csteat8u errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 9.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vhal4lxr with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0028842423466857656\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154543-vhal4lxr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/vhal4lxr' target=\"_blank\">comic-sweep-48</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/vhal4lxr' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/vhal4lxr</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 9.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">comic-sweep-48</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/vhal4lxr' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/vhal4lxr</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154543-vhal4lxr/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run vhal4lxr errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 9.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run vhal4lxr errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 9.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: znt1e3rv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0024123893350894684\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154558-znt1e3rv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/znt1e3rv' target=\"_blank\">worthy-sweep-49</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/znt1e3rv' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/znt1e3rv</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 10.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">worthy-sweep-49</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/znt1e3rv' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/znt1e3rv</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154558-znt1e3rv/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run znt1e3rv errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 10.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run znt1e3rv errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 10.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xw0ulo79 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001859139580716705\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 42\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_batch_norm: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_154614-xw0ulo79</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xw0ulo79' target=\"_blank\">icy-sweep-50</a></strong> to <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/sweeps/wnhzz4d3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xw0ulo79' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xw0ulo79</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 10.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">icy-sweep-50</strong> at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xw0ulo79' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new/runs/xw0ulo79</a><br> View project at: <a href='https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new' target=\"_blank\">https://wandb.ai/da24m004-iitmaana/inaturalist-cnn-sweep_new</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250416_154614-xw0ulo79/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run xw0ulo79 errored:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n    model = model.to(device)\n            ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n    return self._apply(convert)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n    param_applied = fn(param)\n                    ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n    return t.to(\n           ^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 10.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run xw0ulo79 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/2283347234.py\", line 75, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = model.to(device)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 2828 has 14.73 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 10.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data augmentation and normalization\nconfig=None\nwith open('/kaggle/working/best_config.json', 'r') as f:\n    config = json.load(f)\n    print(config)\n    test_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                          std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Load dataset\n    test_dataset = datasets.ImageFolder(\n        root='/kaggle/working/inaturalist_12K/val',\n        transform=test_transform\n    )\n    \n    # Split into train and validation\n    test_size = int(len(test_dataset))\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=4\n    )\n\n\n    \n\n    # Initialize model\n    model = FlexibleCNN(\n        num_filters=config['num_filters'],\n        filter_size=config['filter_size'],\n        activation=config['activation'],\n        dense_neurons=config['dense_neurons'],\n        use_batch_norm=config['use_batch_norm'],\n        dropout_rate=config['dropout_rate']\n    )\n    \n    # Move model to GPU if available\n    device = torch.device(\"cpu\")\n    model = model.to('cpu')\n    # model.load_state_dict(torch.load('best_model.pth'))\n    model.eval()\n    model.to('cpu')\n    \n    # Evaluate on the test set\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images, labels\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    test_accuracy = 100 * correct / total\n    print(f'Test Accuracy on {total} test images: {test_accuracy:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:20:29.954995Z","iopub.execute_input":"2025-04-16T16:20:29.955550Z","iopub.status.idle":"2025-04-16T16:23:38.590801Z","shell.execute_reply.started":"2025-04-16T16:20:29.955523Z","shell.execute_reply":"2025-04-16T16:23:38.589799Z"}},"outputs":[{"name":"stdout","text":"{'activation': 'mish', 'batch_size': 32, 'dense_neurons': 512, 'dropout_rate': 0.2, 'epochs': 10, 'filter_size': 5, 'learning_rate': 0.00242225743608726, 'num_filters': 64, 'seed': 42, 'use_batch_norm': True}\nTest Accuracy on 2000 test images: 9.60%\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# /kaggle/working/inaturalist_12K/train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:13:41.289828Z","iopub.execute_input":"2025-04-15T16:13:41.290214Z","iopub.status.idle":"2025-04-15T16:13:41.294748Z","shell.execute_reply.started":"2025-04-15T16:13:41.290187Z","shell.execute_reply":"2025-04-15T16:13:41.293842Z"}},"outputs":[],"execution_count":39}]}