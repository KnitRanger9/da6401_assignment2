{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport zipfile\nimport requests\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\n# import torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport torchvision.models as models\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import random_split, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nfrom pathlib import Path\nimport json\nimport wandb\nimport time\nimport copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T22:28:27.403992Z","iopub.execute_input":"2025-04-18T22:28:27.404262Z","iopub.status.idle":"2025-04-18T22:28:36.704060Z","shell.execute_reply.started":"2025-04-18T22:28:27.404238Z","shell.execute_reply":"2025-04-18T22:28:36.703499Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"dataset_url = \"https://storage.googleapis.com/wandb_datasets/nature_12K.zip\"\ndataset_zip_path = \"/kaggle/working/nature_12K.zip\"\ndataset_dir = \"nature_12K\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Parameters\nnum_classes = 10  # For iNaturalist (adjust if needed)\nbatch_size = 16\nnum_epochs = 10\nlearning_rate = 1e-3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T22:28:36.704989Z","iopub.execute_input":"2025-04-18T22:28:36.705245Z","iopub.status.idle":"2025-04-18T22:28:36.766758Z","shell.execute_reply.started":"2025-04-18T22:28:36.705219Z","shell.execute_reply":"2025-04-18T22:28:36.766050Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"if not os.path.exists(dataset_dir):\n    if not os.path.exists(dataset_zip_path):\n        print(\"Downloading iNaturalist-12K...\")\n        response = requests.get(dataset_url, stream=True)\n        total_size = int(response.headers.get('content-length', 0))\n        with open(dataset_zip_path, 'wb') as f, tqdm(\n            desc=dataset_zip_path,\n            total=total_size,\n            unit='iB',\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as bar:\n            for data in response.iter_content(chunk_size=1024):\n                size = f.write(data)\n                bar.update(size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T22:28:37.586658Z","iopub.execute_input":"2025-04-18T22:28:37.587166Z","iopub.status.idle":"2025-04-18T22:31:01.129522Z","shell.execute_reply.started":"2025-04-18T22:28:37.587137Z","shell.execute_reply":"2025-04-18T22:31:01.128930Z"}},"outputs":[{"name":"stdout","text":"Downloading iNaturalist-12K...\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/nature_12K.zip: 100%|██████████| 3.55G/3.55G [02:22<00:00, 26.8MiB/s] \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"if not os.path.exists(dataset_dir):\n    print(\"Extracting dataset...\")\n    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n        zip_ref.extractall(\".\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T22:31:01.130605Z","iopub.execute_input":"2025-04-18T22:31:01.130833Z","iopub.status.idle":"2025-04-18T22:31:15.005997Z","shell.execute_reply.started":"2025-04-18T22:31:01.130816Z","shell.execute_reply":"2025-04-18T22:31:15.005259Z"}},"outputs":[{"name":"stdout","text":"Extracting dataset...\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def prepare_datasets(data_dir, val_split=0.2, batch_size=32, image_size=(224, 224)):\n    data_dir = Path(data_dir)\n\n    # Define transforms (customize as needed)\n    transform = transforms.Compose([\n        transforms.Resize(image_size),\n        transforms.ToTensor(),  # Converts to [0, 1] and CxHxW\n        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3),  # Normalize RGB\n    ])\n\n    # Load training and testing datasets\n    full_train_dataset = ImageFolder(root=data_dir / \"train\", transform=transform)\n    test_dataset = ImageFolder(root=data_dir / \"val\", transform=transform)\n\n    # Create validation split from training set\n    val_size = int(val_split * len(full_train_dataset))\n    train_size = len(full_train_dataset) - val_size\n\n    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n\n    # Extract X and Y by iterating over DataLoader batches if needed\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n    # # Convert to X, Y tensors\n    # def extract_XY(loader):\n    #     for X, Y in loader:\n    #         return X, Y\n\n    # X_train, Y_train = extract_XY(train_loader)\n    # X_val, Y_val = extract_XY(val_loader)\n    # X_test, Y_test = extract_XY(test_loader)\n\n    return train_loader, val_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T22:31:15.006807Z","iopub.execute_input":"2025-04-18T22:31:15.007069Z","iopub.status.idle":"2025-04-18T22:31:15.015670Z","shell.execute_reply.started":"2025-04-18T22:31:15.007042Z","shell.execute_reply":"2025-04-18T22:31:15.014834Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# X_train, Y_train, X_val, Y_val, X_test, Y_test = prepare_datasets(\n#     data_dir='/kaggle/working/inaturalist_12K', val_split=0.2, batch_size=64, image_size=(224, 224)\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader, val_loader, test_loader = prepare_datasets(\n    data_dir='/kaggle/working/inaturalist_12K', val_split=0.2, batch_size=64, image_size=(224, 224)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T22:31:15.017562Z","iopub.execute_input":"2025-04-18T22:31:15.018120Z","iopub.status.idle":"2025-04-18T22:31:21.090021Z","shell.execute_reply.started":"2025-04-18T22:31:15.018094Z","shell.execute_reply":"2025-04-18T22:31:21.089485Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n\nmodel = models.resnet50(pretrained=True)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 10)  # Replace last layer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T22:31:21.090667Z","iopub.execute_input":"2025-04-18T22:31:21.090920Z","iopub.status.idle":"2025-04-18T22:31:23.727015Z","shell.execute_reply.started":"2025-04-18T22:31:21.090896Z","shell.execute_reply":"2025-04-18T22:31:23.726341Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 192MB/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Freeze all layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace the final layer (fc) for 10-class classification\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, num_classes)\n\n# Only the final layer's parameters will be updated\nmodel = model.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T22:31:23.727761Z","iopub.execute_input":"2025-04-18T22:31:23.728047Z","iopub.status.idle":"2025-04-18T22:31:24.476465Z","shell.execute_reply.started":"2025-04-18T22:31:23.728021Z","shell.execute_reply":"2025-04-18T22:31:24.475830Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def train(config=None):\n    # Initialize wandb\n    with wandb.init(config=config):\n        config = wandb.config\n        \n        # Initialize best_val_acc at the beginning of the function\n        best_val_acc = 0.0\n\n# Set random seed for reproducibility\n        torch.manual_seed(config.seed)\n        np.random.seed(config.seed)\n\n        # Data augmentation and normalization based on config\n        if config.use_augmentation:\n            train_transform = transforms.Compose([\n                transforms.Resize((224, 224)),\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomRotation(10),\n                transforms.ColorJitter(brightness=0.2, contrast=0.2),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                  std=[0.229, 0.224, 0.225])\n            ])\n        else:\n            train_transform = transforms.Compose([\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                  std=[0.229, 0.224, 0.225])\n            ])\n\n        val_transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                              std=[0.229, 0.224, 0.225])\n        ])\n\n        # Load dataset\n        train_dataset = datasets.ImageFolder(\n            root='/kaggle/working/inaturalist_12K/train',  # Update to match your dataset path\n            transform=train_transform\n        )\n        \n        # Split into train and validation\n        train_indices, val_indices = train_test_split(\n            list(range(len(train_dataset))),\n            test_size=0.2,\n            stratify=train_dataset.targets,\n            random_state=config.seed\n        )\n        \n        train_subset = torch.utils.data.Subset(train_dataset, train_indices)\n        val_subset = torch.utils.data.Subset(train_dataset, val_indices)\n        \n        train_loader = DataLoader(\n            train_subset,\n            batch_size=config.batch_size,\n            shuffle=True,\n            num_workers=4\n        )\n        \n        val_loader = DataLoader(\n            val_subset,\n            batch_size=config.batch_size,\n            shuffle=False,\n            num_workers=4\n        )\n\n        model = models.resnet50(pretrained=True)\n        num_ftrs = model.fc.in_features\n        model.fc = nn.Linear(num_ftrs, 10)  # Replace last layer\n\n        # Freeze all layers\n        for param in model.parameters():\n            param.requires_grad = False\n        \n        # Replace the final layer (fc) for 10-class classification\n        num_ftrs = model.fc.in_features\n        model.fc = nn.Linear(config.num_filters, 10)\n        \n        # Only the final layer's parameters will be updated\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        model = model.to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.fc.parameters(), lr=config.learning_rate)\n\n        # The rest of the training code remains the same\n        for epoch in range(config.epochs):\n            # Training phase\n            model.train()\n            train_loss = 0\n            train_correct = 0\n            train_total = 0\n\n            for batch_idx, (inputs, targets) in enumerate(train_loader):\n                inputs, targets = inputs.to(device), targets.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                loss.backward()\n                optimizer.step()\n\n                train_loss += loss.item()\n                _, predicted = outputs.max(1)\n                train_total += targets.size(0)\n                train_correct += predicted.eq(targets).sum().item()\n\n                if batch_idx % 100 == 0:\n                    print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n\n            train_acc = 100. * train_correct / train_total\n\n            # Validation phase\n            model.eval()\n            val_loss = 0\n            val_correct = 0\n            val_total = 0\n\n            with torch.no_grad():\n                for inputs, targets in val_loader:\n                    inputs, targets = inputs.to(device), targets.to(device)\n                    outputs = model(inputs)\n                    loss = criterion(outputs, targets)\n\n                    val_loss += loss.item()\n                    _, predicted = outputs.max(1)\n                    val_total += targets.size(0)\n                    val_correct += predicted.eq(targets).sum().item()\n\n            val_acc = 100. * val_correct / val_total\n\n            # Log metrics to wandb\n            wandb.log({\n                \"epoch\": epoch,\n                \"train_loss\": train_loss / len(train_loader),\n                \"train_acc\": train_acc,\n                \"val_loss\": val_loss / len(val_loader),\n                \"val_acc\": val_acc\n            })\n\n            # Save best model\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                torch.save(model.state_dict(), 'best_model.pth')\n                print(f\"Best model saved with val_acc: {val_acc:.2f}% at epoch {epoch}\")\n\n            print(f'Epoch: {epoch}, Train Loss: {train_loss/len(train_loader):.4f}, '\n                  f'Train Acc: {train_acc:.2f}%, Val Loss: {val_loss/len(val_loader):.4f}, '\n                  f'Val Acc: {val_acc:.2f}%')\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T22:31:24.477191Z","iopub.execute_input":"2025-04-18T22:31:24.477428Z","iopub.status.idle":"2025-04-18T22:31:28.229696Z","shell.execute_reply.started":"2025-04-18T22:31:24.477412Z","shell.execute_reply":"2025-04-18T22:31:28.228798Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"sweep_config = {\n    'method': 'random',  # or 'grid' or 'bayes'\n    'metric': {\n        'name': 'val_acc',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'learning_rate': {\n            'min': 1e-5,\n            'max': 1e-3\n        },\n        'batch_size': {\n            'values': [32, 64, 128]\n        },\n        'epochs': {\n            'value': 10\n        },\n        'seed': {\n            'values': [42, 2023, 7]\n        },\n        'use_augmentation': {\n            'values': [True, False]\n        },\n        'num_filters': {\n            'values': [512, 1024, 2048]  # This replaces fc layer, must match model.fc.in_features\n        }\n    }\n}\n\nsweep_id = wandb.sweep(sweep_config, project=\"da6401_partB\")  # Replace with your wandb project name\n\n# Run the sweep 50 times|\nwandb.agent(sweep_id, function=train, count=50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T22:35:01.871422Z","iopub.execute_input":"2025-04-18T22:35:01.871971Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}